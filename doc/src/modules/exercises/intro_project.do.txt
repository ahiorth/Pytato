
# #ifdef SOLUTIONS
# #if FORMAT in ("ipynb")
!bc pycod
import numpy as np
import scipy as sp
import scipy.optimize
import scipy.special
import pandas as pd
import matplotlib.pyplot as plt
import pathlib
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from mpl_toolkits.mplot3d import axes3d
from matplotlib import cm
!ec
# #endif

# #endif


##__Summary.__
__Learning objectives.__
* Visualize data using "`matplotlib`":"https://matplotlib.org".
* Combine and manipulate data from different input (Excel) files.
* Use "Taylor's theorem":"https://en.wikipedia.org/wiki/Taylor%27s_theorem" to approximate numerical derivatives.
* Study the impact of numerical errors on computed derivatives.
* Fit straight lines to data using the method of least squares.
* Compare your own implementation of the least squares algorithm with functions in available Python libraries.
* (Optional) Implement and use multiple linear regression to fit a linear model to a dataset in which the 'true model' is known to be non-linear.

The work done to answer the exercises should be documented clearly in a
handed-in Jupyter notebook.

TOC: on

!split
<% ex_id = 0 %>
<% ex_id += 1 %>
======= Exercise ${ex_id}: Visualize oil production data =======
<% exercise_plot_func = ex_id %>

<% counter = 0 %>
<% counter += 1 %>

When analyzing data it is very advantageous to start by making a plot.
The human mind is usually good at detecting patterns, and by looking at
the data one can think of simple ideas to test out, before possibly
doing a more comprehensive analysis.

As part of this project we will look at some of the datasets that
are available at the Norwegian Petroleum Directorate (NPD)
"website":"http://factpages.npd.no/factpages/".
These data are updated regularly, and if you are able to make a good model
of, e.g., the historical oil production versus time, you could use it to
forecast the production in the future as well.

<% counter = 1 %>
__Part ${counter}.__
<% ex_df_field = counter %>

A challenge when doing data analysis is that the data is usually not in the
format you would like it to be in; it first needs to be manipulated somehow.
In Python there are many ways of achieving this, but the
"`Pandas`":"https://pandas.pydata.org" library is particularly well-suited
for it.
Below is a code snippet to help you to get started with reading data from Excel
into a `Pandas` "`DataFrame`":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html":

!bc pypro
import numpy as np
import pandas as pd
import pathlib
import matplotlib.pyplot as plt

def df_field(name, datafile='field_production_gross_monthly.xls', col=0):
    ## to do: fix location of files.. (parent, data/Input etc...)
    folder = pathlib.Path.cwd().parent.joinpath('data')
    filename = folder.joinpath(datafile)
    df = pd.read_excel(filename)
    columns = df.columns
    return df[df[df.columns[col]] == name]
!ec

Note that all data files needed for this project are located in the `data`
folder.

* Explain in your own words what the above Python function does.
* Open the file `field_production_gross_monthly.xls`, and compare its contents with the output from the command `print(df_field('OSEBERG'))`.
##* Modify the code so that it also returns the columns names, `columns`.

<% counter += 1 %>
__Part ${counter}.__

Next, we wish to look at the historical oil production versus time for a
specific field. Such plots are already available at the
"NPD website":"http://factpages.npd.no/factpages/", but here you are going to
make your own by using the "`matplotlib`":"https://matplotlib.org" library.

When we develop code, it is good practice to use functions to divide a
problem into smaller pieces. We start by extracting oil production data
for a specific field with the function already defined in
Part ${ex_df_field}:

!bc pypro
def prod_data(name):
    df = df_field(name)
    columns = df.columns
    Year    = df[ ... ]
    Month   = df[ ... ]
    OilProd = df[ ... ]
    #Assume 30 days in each month and 365 in year
    Year    = Year + Month*30/365
    return Year, OilProd
!ec

* Complete writing the above function.

<% counter += 1 %>
__Part ${counter}.__
* We also make a function that plots the oil production versus time:

!bc pypro
def plot_prod_data(name):
    Year, OilProd=prod_data(name)
    ...  # make plot here
!ec
* Finish implementing this Python function as well.

Calling `plot_prod_data('OSEBERG')` should generate a figure that looks
something like this:

FIGURE: [fig-intro_project/oseberg.png, width=400 frac=1.0]

<% counter += 1 %>
__Part ${counter} (Optional).__
In the oil production plot for a given field, we additionally want to include
information about the number of wells that have been drilled. Specifically,
we wish to plot, on a separate $y$-axis, the cumulative number of wells present
in the field at any given moment in time.

Well data is stored in the file `wellbore_development_all.xls`.
In this file, each row entry corresponds to a single well, and the
field to which it belongs is stored in column with index 14. Hence, we can
start by writing:

!bc pypro
def plot_prod_and_well_data(name):
    fn = 'wellbore_development_all.xls'
    df = df_field(name, datafile=fn, col=14)
    columns = df.columns
    year = df['Completed year']  # alternatively: fetch by index (32)
    ...
!ec

Note that the Excel file only tells you the year in which each well was
completed; *there could be many wells drilled a given year, and the list is
not sorted*. This means that you need to a little more work to process the
data:

* Finish implementing the above Python function.
* Calling it with a specific field as input should generate a plot that looks something like this:

FIGURE: [fig-intro_project/oseberg_wells.png, width=400 frac=1.0]

If you want, you can also distinguish between different types of wells, i.e.,
between injection/production/observation wells.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_
to do....
# #endif
## @@@CODE-4 src-intro_project/draugen_pandas.py fromto: import numpy@name = 'DRAUGEN'

<% ex_id += 1 %>
======= Exercise ${ex_id}: More wells, more oil? =======
<% ex_more_wells_oil = ex_id %>

A very common statement is "more wells - more oil". The fields on the Norwegian
Continental Shelf (NCS) are in different stages of production, and they have
varying reservoir properties, so a thorough analysis would take time.
However, we would at least expect to see some correlation between the
*total amount of oil produced* and *the number of wells drilled*, if the
hypothesis is correct.

<% counter = 0 %>
<% counter += 1 %>
__Part ${counter}.__
The first task is to fetch the (final) number of wells drilled for each field,
as well as the total (cumulative) oil production. The resulting values are
to be stored in arrays:

* The hard way is to do this yourself by extracting the data you need from a combination of the provided NPD `.xls` files.
* Alternatively, you can 'cheat' and use the preprocessed data stored in `fields_oil_wells.xlsx`.
## in the `data` folder.

<% counter += 1 %>
__Part ${counter}.__
<% ex_plot_eo_wells = counter %>

* Make a scatter plot showing the cumulative oil production of all the fields on the $y$-axis. On the $x$-axis, plot the number of wells for the fields (you might want to exclude observation wells).

<% counter += 1 %>
__Part ${counter}.__
The Draugen field is very homogeneous, and it is therefore regarded as an ideal
field on the NCS. It also has an active aquifer underlying the reservoir,
meaning that when producing oil by water injection the aquifer provides
additional pressure support.
##The Draugen field has one of the highest recoveries on the NCS.

Does the plot you made in Part ${ex_plot_eo_wells} indicate that Draugen
has been a successful field compared to the others? Why/why not?

# #ifdef SOLUTIONS
_POSSIBLE SOLUTION_
to do....
##@@@CODE src-intro_project/draugen_pandas.py
# #endif

<% ex_id += 1 %>
======= Exercise ${ex_id}: Finite differences =======
<% exercise_fdiff = ex_id %>

The most straightforward way to approximate the derivative of a function
$f=f(x)$ is to use the function value at $x$ *and* at a small distance $h$
from $x$, e.g.:
!bt
\begin{equation}
f^{\prime}(x)\approx\frac{f(x+h)-f(x)}{h}\,.
\end{equation}
!et
This approximation is called the *forward difference*.
For 'well-behaved' functions, it can be shown using Taylor's formula
that
!bt
\begin{equation}
\label{eq:fder_fd}
f^{\prime}(x)=\frac{f(x+h)-f(x)}{h}-h\frac{f^{\prime\prime}(\xi)}{2}\,,
\end{equation}
!et
for some $\xi$ between $x$ and $x+h$.

<% counter = 0 %>
<% counter += 1 %>
__Part ${counter}.__
Explain in detail how equation (ref{eq:fder_fd}) can be derived from Taylor's
formula.

What is the order of the truncation error for the forward difference numerical
differentiation method? What should ideally happen if you lower the step size
by a factor of ten?
## If you reduce the step size by a factor of ten, how do you expect the error to change?

<% counter += 1 %>
__Part ${counter}.__
Alternatively, we could use the *backward difference* approximation:
!bt
\begin{equation}
f^{\prime}(x)\approx\frac{f(x)-f(x-h)}{h}\,.
\end{equation}
!et
For this method, use Taylor expansions to derive a similar expression
as (ref{eq:fder_fd}).

What is the truncation error for this method?
##If you reduce the step size by a factor of ten, how do you expect the error
##to change for this method?

# #ifdef SOLUTIONS

_SOLUTION TO PREVIOUS TWO EXERCISES_

From Taylor's formula we have that:
!bt
\begin{equation}
f(x+h)=f(x)+hf^{\prime}(x)+h^{2}\frac{f^{\prime\prime}(x)}{2}+\ldots
+ h^{n}\frac{f^{(n)}(x)}{n!}+
h^{n+1}\frac{f^{(n+1)}(\xi)}{(n+1)!}\,,
\end{equation}
!et
for some $\xi$ between $x$ and $x+h$. The desired formula is obtained from
a) using $n=2$, b) moving $f(x)$ over to the left-hand side, and c) dividing
by $h$.

An entirely similar expression is found for the backward difference method
by replacing $h$ by $-h$. Then, we eventually find:

!bt
\begin{equation}
\label{eq:fder_bd}
f^{\prime}(x)=\frac{f(x)-f(x-h)}{h}-h\frac{f^{\prime\prime}(\xi)}{2}\,,
\end{equation}
!et
for some $\xi$ between $x-h$ and $x$.

Both methods are accurate as $\mathcal{O}(h)$, meaning that if we decrease
the step size by an order of magnitude, the truncation error is expected to
change in the same way.
# #endif

<% counter += 1 %>
__Part ${counter}.__
The *central difference* approximation of the derivative is given by
!bt
\begin{equation}
f^{\prime}(x)\approx\frac{f(x+h)-f(x-h)}{2h}\,.
\end{equation}
!et
Again, use Taylor expansions to derive this expression. What is the truncation
error, and how does it compare to the other two methods?
(hint: combine the derivations you did for the forward and backward
finite difference schemes)

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

We can derive this formula by combining the two Taylor expansions found
in the previous part. More specifically, we start by using $n=3$ when
expanding $f(x+h)$ and $f(x-h)$, and then we subtract the former from the
latter, yielding:
!bt
\begin{equation}
f^{\prime}(x)=\frac{f(x+h)-f(x-h)}{2h}-\frac{h^2}{12}\cdot
\left(f^{\prime\prime\prime}(\xi_1)+f^{\prime\prime\prime}(\xi_2)\right)\,,
\end{equation}
!et
for some $\xi_1$ between $x$ and $x+h$, and $\xi_2$ between $x-h$ and $x$.
Assuming $f^{\prime\prime\prime}$ to be continuous, the intermediate value
theorem from calculus tells us that there must exist a point $\xi$ between
$\xi_1$ and $\xi_2$ for which:
!bt
\begin{equation}
f^{\prime\prime\prime}(\xi)=
\frac{f^{\prime\prime\prime}(\xi_1)+f^{\prime\prime\prime}(\xi_2)}{2}\,.
\end{equation}
!et
Consequently, we find:
!bt
\begin{equation}
\label{eq:fder_cd}
f^{\prime}(x)=\frac{f(x+h)-f(x-h)}{2h}-\frac{h^2}{6}\cdot
f^{\prime\prime\prime}(\xi)\,.
\end{equation}
!et

The truncation error goes like $\mathcal{O}(h^2)$, meaning that decreasing
$h$ by a factor 10 should decrease the error by a factor 100.
# #endif

<% counter += 1 %>
__Part ${counter}.__
Implement a Python function that can calculate numerical derivatives of
functions $f=f(x)$. The routine should be able to handle all of the three
aforementioned formulas (forward difference, backward difference, central
difference), as well as different values of the step size, $h$.

You may take the following code as a starting point:

!bc pycod
def numerical_derivative(f, x, *, method='forward', h=1.0e-4):
    """
    Function to numerically evaluate the derivative of a function
    f=f(x).

    :param f: Function to differentiate.
    :param x: Point at which to evaluate the derivative.
    :param method: Approximation formula to apply. Available options
                   are 'forward' (default), 'backward', and 'central'
                   finite differences.
    :param h: Step size (default: 1.0e-4).
    :return: Approximate value for f'(x).
    """
!ec
## @@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: def numerical_derivative@if method.upper
# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_
@@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: def numerical_derivative@-----

# #endif

<% counter += 1 %>
__Part ${counter}.__
<% ex_plot_fd_errors = counter %>
Consider the function
!bt
\[
f(x)=\sqrt{x^2+5}\,.
\]
!et
## f(x)=\exp(-x)\cdot\sin(x)

We want to estimate the numerical error when calculating $f^\prime(1)$ with
a finite difference formula:

* First, show that the exact value of the derivative is $f^\prime(1)=\frac{1}{\sqrt{6}}$.
* Apply your implemented Python routine to numerically evaluate $f^{\prime}(1)$. Do this for $h=10^{-16}, h=10^{-15}, \ldots$, up to $h=10^{-1}$, and for all of the three introduced finite difference formulas.
* For each differentiation method, make a figure showing the absolute value of the error versus $h$ on a double-logarithmic plot.
##What do you see?

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_
@@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: Use a simple function with well-known derivative@-----

@@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: Plot numerical errors of finite difference formulas@-----

# #endif

<% ex_id += 1 %>
======= Exercise ${ex_id}: Fitting a simple linear model to data =======

When dealing with physical data points, they rarely fit a smooth curve, but
we would still like to calculate derivatives to report, e.g., how
fast a quantity is changing as a function of time. One way of achieving this
is to calculate the derivative directly from the data, using a numerical
differentiation formula. However, such an approach can be very sensitive to
the choice of points at which we evaluate the derivative.

Another approach is to fit a straight line to the data, and report the slope
of the line. Suppose we are given $N$ data points ($x_i$, $y_i$), and that
we wish to approximate $y$ (the dependent variable) as a linear function of
$x$ (the independent variable), i.e.:
!bt
\[
y\approx \alpha+\beta\cdot{x}\,.
\]
!et
One common algorithm for doing this is the method of
"least squares":"https://en.wikipedia.org/wiki/Least_squares".
The idea here is to select the slope $\beta$ and the intercept $\alpha$ in
such a way that the sum of squares of distances from the line to the data
points are minimized. Mathematically, we need to make the following
expression as small as possible:

!bt
\begin{equation}
\label{eq:chi_squared}
\chi^2=\displaystyle\sum_{i=1}^{N}\left(y_i-\alpha-\beta\cdot{x_i}\right)^2\,.
\end{equation}
!et
## #if FORMAT in ("latex, pdflatex")
##FIGURE:[src-intro_project/figs_python/OLS.png, width=600 frac=0.8] Example of least squares fit to a data set. label{fig:OLSR}
## #endif

To find the minimum, we can differentiate (ref{eq:chi_squared}) with respect
to both $\alpha$ and $\beta$, and set the resulting expressions equal to zero.
If we do that, it is possible to show that we must have
!bt
\begin{equation}
\label{eq:OLSR_intercept}
\alpha=\overline{y}-\beta\overline{x}\,,
\end{equation}
!et
and
!bt
\begin{equation}
\label{eq:OLSR_slope}
\beta=\frac{\displaystyle\sum_{i=1}^{N}(x_i-\overline{x})(y_i-\overline{y})}
{\displaystyle\sum_{i=1}^{N}(x_i-\overline{x})^2}
\end{equation}
!et
in which $\overline{x}$ and $\overline{y}$ are arithmetic averages of
the $x-$ and $y$-values:
!bt
\begin{align}
\overline{x} &= \frac{1}{N}\displaystyle\sum_{i=1}^{N} x_{i} \\
\overline{y} &= \frac{1}{N}\displaystyle\sum_{i=1}^{N} y_{i} \,.
\end{align}
!et
The algorithm is also referred to as *ordinary least squares
regression* (OLSR), or *simple linear regression*. It is a standard
technique used in statistical analyses of empirical data.
Often, the goal is to be able to predict the outcome $y$ based on a
knowledge of $x$.

<% counter = 0 %>
<% counter += 1 %>
__Part ${counter}.__

* Finish the implementation of the following Python function:
@@@CODE src-intro_project/pINTRO_linear_regression.py fromto: def OLSR@x_avg

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_
@@@CODE src-intro_project/pINTRO_linear_regression.py fromto: def OLSR@-----

# #endif

<% counter += 1 %>
__Part ${counter}.__
* Test the correctness of your implementation by applying the function to a fabricated data set of your own choosing in which the relationship between $x$ and $y$ is exactly linear.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

@@@CODE-4 src-intro_project/pINTRO_linear_regression.py fromto: Test least squares routine@-----

# #endif

<% counter += 1 %>
__Part ${counter}.__

The goodness of fit of a statistical model is frequently evaluated by
considering the
"coefficient of determination":"https://en.wikipedia.org/wiki/Coefficient_of_determination",
or $R^2$ (*R-squared*). The purpose of $R^2$ is to measure the proportion of
variation in the dependent variable that can be explained by variation in the
independent variable(s). For the cases investigated in this project, the
relevant formula is
!bt
\begin{equation}
R^2=1-\frac{\displaystyle\sum_{i=1}^{N}(y_i-\hat{y_i})^2}
{\displaystyle\sum_{i=1}^{N}(y_i-\overline{y})^2}\,,
\end{equation}
!et
in which $\hat{y_i}$ is the prediction of the model for data point $i$.
For least squares regression models it can be shown that $R^2=\rho^2$,
where $\rho$ is
"Pearson's correlation coefficient":"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient".
## How to go from def. in terms of variance to actual formula?
## Assumptions on distribution of residuals??
* Fill in the missing details of the following Python function:
@@@CODE src-intro_project/pINTRO_linear_regression.py fromto: def calc_r_squared@1.0

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

We can for example using available functions from the NumPy library to make
the computations easier:

@@@CODE src-intro_project/pINTRO_linear_regression.py fromto: def calc_r_squared@-----

Here, np.mean(a) computes the arithmetic average of all values in an array a,
while np.sum(a) computes the sum.

# #endif

<% counter += 1 %>
__Part ${counter}.__
We will now investigate the scatter plot you made in Part ${ex_plot_eo_wells}
of Exercise ${ex_more_wells_oil} in a little more detail:
* Extend the plot by including in it a) the best fit straight line to the data in the least squares sense, and b) Pearson's correlation coefficient for the fit.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

to do...

# #endif

<% counter += 1 %>
__Part ${counter}.__

* Calculate the slope and intercept of the least squares line again, but this time use available function(s) from established Python libraries.

As an example, you might opt for the
"`LinearRegression`":"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
class from the
"`scikitlearn`":"https://scikit-learn.org/", library or perhaps the polynomial
interpolation function `polyfit` available in
"`NumPy`":"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyfit.html".
Whatever you choose, make sure that you read the documentation and explain in
your own words what you do as you go along. Moreover:
* Check that you get the same regression line as when using your own implementation.

<% ex_id += 1 %>
======= Exercise ${ex_id}: Error analysis for finite difference schemes =======

<% counter = 0 %>
<% counter += 1 %>
__Part ${counter}.__
Consider again the error plots you made for the three finite difference
formulas in Part ${ex_plot_fd_errors} of Exercise ${exercise_fdiff}.
In each case, fit a straight line to the right part of the error curve.
How do the resulting slopes compare to what you would expect from a
theoretical truncation error analysis?

Also fit a straight line to the left part of each error curve.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

!bc pycod
# Define functions again (to avoid name clashes):
def f(x):
    return np.sqrt(x**2+5.0)

# Also define first and second derivatives of f (for later use):
def df(x):
    return x/np.sqrt(x**2+5.0)

def df2(x):
    return 5.0/(x**2+5.0)**(3.0/2)
!ec

@@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: Fit straight lines to the finite difference error plots@-----

We note that both the forward difference method and the backward difference
method has a computed error proportional to $h$, as long as we restrict
the step size to about $h=10^{-8}$ and upwards. Around this
value the error seems to be at a minimum, but as $h$ is lowered even further
the error starts to rise again. We might expect the error to go as roughly
$\mathcal{O}(1/h)$, which is relatively close to what we find (there is
some scatter in the values).

For the central difference method, the error declines with $h^2$ initially,
but reaches its minimum point earlier (somewhere in-between $10^{-6}$ and
$10^{-5}$), after which the error again seems to increase as
$\mathcal{O}(1/h)$.

The right part of the curves can be understood as a consequence of the
truncation error, i.e., it is predicted by Taylor's formula.
On the other hand, the left part shows that as the step size becomes
increasingly small, round-off errors start to dominate the numerical results.

# #endif

<% counter += 1 %>
__Part ${counter}.__
By taking absolute values, it follows from (ref{eq:fder_fd}) that
the truncation error $E(f;x_0;h)$ of the forward difference approximation
to the derivative at $x=x_0$ must be bounded above by
!bt
\begin{equation}
\label{eq:fder_fd_upper_bound_abs_error}
E(f;x_0;h)\leq\frac{h}{2}\cdot\text{max}_{x\in[x_0,x_0+h]}|f^{\prime\prime}(x)|\,.
\end{equation}
!et
For the function $f(x)=\sqrt{x^2+5}$ and the selected point $x_0=1$, explain
why the truncation error will at most be
!bt
\begin{equation}
E_{\text{max}}\approx{0.17h}\,.
\end{equation}
!et
## E_{\text{max}}\sim{h}\exp(-1)\cos(1)\approx{0.198766103h}\,.
Compare this with the actual error found using your differentiation routine,
and comment upon the result.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

The function $f^{\prime\prime}$ is positive in the vicinity of $x_0=1$, and
it is monotonically decreasing there,
!bt
\begin{align*}
f^{\prime\prime}(x) &= \frac{5}{(x^2+5)^{3/2}} > 0 \\
f^{\prime\prime\prime}(x) &= -\frac{3x}{5(x^2+5)^{5/2}} < 0 \,,
\end{align*}
!et
hence $f^{\prime\prime}(1)$ provides an upper bound for the second derivative
on the interval $[x_0, x_0+h]$.
Inserting into (ref{eq:fder_fd_upper_bound_abs_error}) yields the desired
error estimate:
!bt
\begin{align*}
E_{\text{max}}=\frac{h}{2}\cdot\frac{5}{6^{3/2}}\approx{0.17h}\,.
\end{align*}
!et

@@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: Investigate truncation errors in more detail@-----

We see that the estimated maximal error is very close to the actual error for
the highest $h$-values. However, it eventually breaks down as $h\rightarrow{0}$,
again because of round-off errors.
# #endif

<% counter += 1 %>
__Part ${counter}.__
Add an extra term to the estimate for the error as follows:
!bt
\begin{equation}
E_{\text{max}}\approx{0.17h}+\frac{2\epsilon_M}{h}|f(1)|\,.
\end{equation}
!et
You may assume that $\epsilon_{M}\sim{10^{-16}}$. Redo the comparison you made
in the previous exercise. What is different now? Explain the meaning of the
second term in the above formula (hint: you may want to read about the
"machine epsilon":"https://en.wikipedia.org/wiki/Machine_epsilon")

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

The second term provides an estimate for the round-off error due to finite
precision. For a 64-bit number system, it can be shown that as long as no
underflow or overflow occurs, the computed value $\overline{f(x_0)}$
satisfies
!bt
\begin{equation}
\overline{f(x_0)}=f(x_0)(1+\epsilon_1)\,,
\end{equation}
!et
for $|\epsilon_1|\leq{\epsilon_M}$. Similarly,
!bt
\begin{equation}
\overline{f(x_0+h)}=f(x_0+h)(1+\epsilon_2)\,,
\end{equation}
!et
where $|\epsilon_2|\leq{\epsilon_M}$. Hence, the calculated forward
difference compares to the exact value (i.e., without round-off errors)
as follows:
!bt
\begin{equation}
\frac{\overline{f(x_0+h)}-\overline{f(x_0)}}{h}=\frac{f(x_0+h)-f(x_0)}{h}+
\frac{f(x_0+h)\epsilon_2-f(x_0)\epsilon_1}{h}\,.
\end{equation}
!et

For very small $h$, we approximate $f(x_0+h)\approx{f(x_0)}$, and so an
estimate for the round-off error is
!bt
\begin{equation}
\frac{\epsilon_2-\epsilon_1}{h}f(x_0)\,,
\end{equation}
!et
and in the worse case scenario we obtain the following upper bound for the
absolute value $E_R$ of the round-off error:
!bt
\begin{equation}
E_R\sim\frac{2\epsilon_M}{h}|f(x_0)|\,.
\end{equation}
!et

Let us test this for our example at hand:

@@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: Add round-off error term@-----

We see that we now get a good estimate of the total error across the entire
range of step sizes!
# #endif

<% counter += 1 %>
__Part ${counter}.__
An estimate for the value of $h$ at which the total error reaches its
minimum is given by:
!bt
\begin{equation}
h_{min}\approx{2}\sqrt{\frac{|\epsilon_{M}f(x_0)|}{|f^{\prime\prime}(x_0)|}}\,.
\end{equation}
!et
Explain how to obtain this formula from the above estimate of the total error.
For the currently investigated function and $x_0$, calculate $h_{min}$ and
compare it with the plotted results.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

The formula is obtained by differentiating the expression for the maximal
total error with respect to step size $h$, setting it to zero, and solving
for $h$. When we plot the result for our particular case, we get:

@@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: Estimate critical value of the step size@-----
# #endif

<% counter += 1 %>
__Part ${counter}.__
Use the forward difference method to estimate the derivative $f^{\prime}(x_0)$
again, but this time ensure that Python uses 32 bits to represent floating
point numbers. Make a double-logarithmic plot of the error versus step size,
in which you compare the results with what you found for the default
number representation (64 bits).

What do you see? Approximately what value should you employ for $\epsilon_M$
now?

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

We see that round-off errors start to become important much earlier than
in the 64-bit case:

@@@CODE-4 src-intro_project/pINTRO_first_project.py fromto: Investigate effect of using 32-bit floating point numbers@-----

The reason is that we have much lower precision in representing numbers
as floats. Indeed, we have already seen that for a 32-bit representation,
the machine epsilon is $\epsilon_M\sim{1.0e-8}$, as opposed to
$\sim{1.0e-16}$ for the 64-bit case.

For small enough values of $h$, the error stabilizes at a constant
value equal to the true derivative $|f^{\prime}(1)|$, as all numerical
derivatives are evaluated to zero; this is because the computer cannot
distinguish between the values of $f(x+h)$ and $f(x)$.
# #endif

<% ex_id += 1 %>
======= Exercise ${ex_id}: (Optional) Multiple linear regression =======
label{linear_regression_normal_eqs}

Previously we looked at data consisting of tuples of numbers $x$ and $y$,
and we attempted to fit a straight line to describe $y$ as a function of $x$.
In reality there tend to be multiple factors affecting the outcome of an
experiment, which means that $y$ should be a function of more than a single
independent variable.

To generalize, suppose therefore that we have $k>1$ independent variables,
so that each observational data point can be viewed as a vector in
$k+1$-dimensional space: $k$ x-values, and a single $y$-value.
We still want to try and capture the relationship between $y$- and the
$x$-variables in terms of a linear model, i.e., an expression of the form
!bt
\begin{equation}
y\approx\beta_0 + \displaystyle\sum_{j=1}^{k}\beta_{j}x_{j}\,,
\end{equation}
!et
where $x_1$, $x_2$, $\ldots$, $x_k$ are $k>1$ variables in
question.

Let $x_{i,j}$ denote the collected value of $x_j$ for observation $i$.
As before, the ordinary least squares solution consists in minimizing
the sum of squared residuals. Given a set of 'training data', the expression
to be minimized is
!bt
\begin{equation}
\label{eq:chi_squared_multilinear}
\chi^2=\displaystyle\sum_{i}\left(y_i-\beta_0-
\displaystyle\sum_{j=1}^k\beta_{j}x_{i,j}\right)^2\,.
\end{equation}
!et

By differentiating (ref{eq:chi_squared_multilinear}) with respect to each of
the $\beta$'s, setting the obtained expressions to zero, and solving the
resulting linear system of equations, we find an estimated set of 'best fit'
coefficients $\hat{\beta_0}$, $\hat{\beta_1}$, $\ldots$, $\hat{\beta_k}$.
This system of equations can also be derived in an alternative fashion, using
theory from linear algebra. First, note that the expression to be
minimized can be rewritten in matrix-vector notation as
!bt
\begin{align}
\chi^2=\langle \vec{\epsilon}, \vec{\epsilon} \rangle=\|\vec{y}-X\vec{\beta}\|^2\,,
\end{align}
!et
where $\vec{\epsilon}=(\epsilon_1, \ldots, \epsilon_{n})^{\top}=\vec{y}-X\vec{\beta}$
is the vector of residuals, $\vec{y}=(y_1, \ldots, y_{n})^{\top}$,
$\vec{\beta}=(\beta_0, \ldots, \beta_{k})^{\top}$, and where $X$ is the
following matrix:

!bt
\begin{equation}
X=
\left[ {\begin{array}{ccccc}
 1 & x_{1,1} & x_{1,2} & \ldots & x_{1,k} \\
 1 & x_{2,1} & x_{2,2} & \ldots & x_{2,k} \\
 \text{ } & \text{ } & \ldots & \text{ } & \text{ } \\
 1 & x_{n,1} & x_{n,2} & \ldots & x_{n,k} \\
\end{array} } \right]\,.
\end{equation}
!et
Note that if we want to neglect the constant term ($\beta_0=0$), we can simply
remove the first column of the matrix $X$.

Using the language of linear algebra, we can interpret the least squares
problem geometrically.
We start by noting that the vector $\vec{y}$ is almost certainly not spanned
by the columns of the matrix $X$; otherwise the residual would be zero, and
we would have a perfect linear model. Let $\vec{p}$ be the projection of
$\vec{y}$ onto the column space Col(X), so that the residual vector becomes
\[
\vec{\epsilon}=\vec{y}-\vec{p}=\vec{y}-X\hat{\beta}\,,
\]
for some vector $\hat{\beta}$. By definition, the projection is the vector
in Col(X) with the smallest Euclidean distance away from $\vec{y}$, which
means that $\hat{\beta}$ is the least squares solution! Also, the residual
vector is normal to the column space, meaning that the dot product between
it and any vector from the column space is zero,
\[
\langle X\beta, \vec{\epsilon} \rangle = \langle X\beta, \vec{y}-X\hat{\beta} \rangle
=\beta^{\top}X^{\top}(\vec{y}-X\hat{\beta})=0
\]
for *all* vectors $\vec{\beta}$. Since it is true for all vectors, we
immediately deduce that
\[
X^{\top}(\vec{y}-X\hat{\beta})=0\,,
\]
from which it follows that
!bt
\begin{equation}
\label{eq:normal_eqs_least_squares}
X^{\top}X\hat{\beta}=X^{\top}\vec{y}\,.
\end{equation}
!et
Equations (ref{eq:normal_eqs_least_squares}) are known as the *normal equations*,
and the system can readily solved as long as the matrix $X^{T}X$ is
invertible, which will be the case if and only if the columns of $X$ are
linearly independent.

# #ifdef EXTRA_INFO

_EXTRA EXPLANATION_
For the particular case when $k=1$ (i.e., simple linear regression), the
normal equations become:

!bt
\begin{align*}
\left[ {\begin{array}{cc}
N & \displaystyle\sum_{i=1}^N x_i \\
\displaystyle\sum_{i=1}^N x_i & \displaystyle\sum_{i=1}^N x_{i}^2 \\
\end{array} } \right]\cdot
\left[ {\begin{array}{c}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{array} } \right] =
\left[ {\begin{array}{c}
\displaystyle\sum_{i=1}^N y_i \\
\displaystyle\sum_{i=1}^N x_{i}y_{i}
\end{array} } \right]
\,,
\end{align*}
!et
and using the well-known closed form expression for the inverse of a 2x2
matrix, we calculate:
!bt
\begin{align*}
\hat{\beta}_1&=\frac{N\displaystyle\sum_{i=1}^{N} x_{i}y_{i}-\displaystyle\sum_{i=1}^N x_{i} \displaystyle\sum_{i=1}^N y_i}
{N\displaystyle\sum_{i=1}^N x_{i}^2-\left(\displaystyle\sum_{i=1}^N x_i\right)^2}\\
&= \frac{\displaystyle\sum_{i=1}^N x_{i}y_{i}-\overline{y}\displaystyle\sum_{i=1}^N x_{i}}
{\displaystyle\sum_{i=1}^N x_{i}^2-\overline{x}\displaystyle\sum_{i=1}^N x_{i}}
\end{align*}
!et

To get to equation (ref{eq:OLSR_slope}), we first note that
!bt
\begin{align*}
N\overline{x}^2 &=N\overline{x}\frac{1}{N}\displaystyle\sum_{i=1}^N x_i
= \displaystyle\sum_{i=1}^N x_{i}\overline{x}\text{, and} \\
N\overline{x}\overline{y} &=N\overline{x}\frac{1}{N}\displaystyle\sum_{i=1}^N y_i
= \displaystyle\sum_{i=1}^N y_{i}\overline{x}
\,,
\end{align*}
!et
from which we get
!bt
\begin{align*}
\displaystyle\sum_{i=1}^N (\overline{x}^2-x_{i}\overline{x})=0\,,
\end{align*}
!et
and
!bt
\begin{align*}
\displaystyle\sum_{i=1}^N (\overline{x} \overline{y}-x_{i}\overline{x})=0\,.
\end{align*}
!et
Adding the first expression to the denominator of the solution $\hat{\beta}_1$,
and the second to the numerator, we finally obtain:
!bt
\begin{align*}
\hat{\beta}_1&= \frac{\displaystyle\sum_{i=1}^N (x_{i}y_{i}- x_{i}\overline{y})+
\displaystyle\sum_{i=1}^N (\overline{x} \overline{y}-x_{i}\overline{x})}
{\displaystyle\sum_{i=1}^N (x_{i}^2-x_{i}\overline{x}) +
\displaystyle\sum_{i=1}^N (\overline{x}^2-x_{i}\overline{x})} \\
&= \frac{\displaystyle\sum_{i=1}^N (x_i-\overline{x})(y_i-\overline{y})}
{\displaystyle\sum_{i=1}^N (x_i-\overline{x})^2} \\
&= \frac{\text{Cov}(x,y)}{\text{Var}(x)}\,.
\end{align*}
!et
## end extra explanation
# #endif

<% counter = 0 %>
<% counter += 1 %>
__Part ${counter}.__
* Implement a custom Python routine in which you solve the ordinary least squares problem for multiple independent variables via the normal equations approach.

When it comes to actually solving the system (ref{eq:normal_eqs_least_squares}),
you may use, e.g.,
the `solve` method in
"scipy.linalg package":"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.solve.html".

You may take the following code as a starting point:

@@@CODE src-intro_project/pINTRO_linear_regression.py fromto: def OLSR_multiple@num_rows

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

@@@CODE src-intro_project/pINTRO_linear_regression.py fromto: def OLSR_multiple@-----

# #endif

<% counter += 1 %>
__Part ${counter}.__

* Implement a Python function that takes as input a matrix of $x$-values and a given choice of fitted regression coefficients, and returns a prediction for the corresponding $y$-values.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

@@@CODE src-intro_project/pINTRO_linear_regression.py fromto: def OLSR_multiple_predict@-----

# #endif

<% counter += 1 %>
__Part ${counter}.__

* Test your first routine by feeding in an $y$-variable which is an exact linear function of the $x$-variables, and check that you get back the known answer for the regression coefficients. For example:

!bc pycod
X = np.array([[1, 1], [1, 2], [2,2], [2,3]])
y = np.dot(X, np.array([1,2]))+3  # y = 3 + x1 + 2*x2
regr_coeffs = OLSR_multiple(X, y)
# more code here..
!ec

##Then, use the second routine to calculate a 'prediction' for $y$.
##How should that compare to the actual $y$? What value should the coefficient
##of determination $R^2$ take on?

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_

Since $y$ is a known linear function of the predictor variables, a correct
implementation should yield $y_{\text{pred}}=y$ and $R^2=1$, e.g.:

@@@CODE-4 src-intro_project/pINTRO_linear_regression.py fromto: Test multiple linear regression routine for a known linear function@-----
# #endif

<% counter += 1 %>
__Part ${counter}.__
* Perform a multiple linear regression (in the least squares sense) on the data from the file *xyz\_data.dat*, assuming that $z$ is essentially a linear function of $x$ and $y$. For data point $i$, let
!bt
\begin{equation}
z_i=\beta_{1}\cdot{x_i}+\beta_{2}\cdot{y_i}+\epsilon_i\,.
\end{equation}
!et
Notice that we do not include a constant term here.

* Report the 'best-fit' estimate for the regression coefficients $\hat{\beta}_1$ and $\hat{\beta}_2$, as well as the value of *R-squared* for the fit.
* Again, to test your implementation you might want to compare with results from another Python library.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_
@@@CODE-4 src-intro_project/pINTRO_linear_regression.py fromto: Fit a multiple linear regression model to the xyz-data@-----

We see that we get a very high value for the coefficient of determination,
and a good visual fit to the data.

# #endif

<% counter += 1 %>
__Part ${counter}.__
* Read another set of data from the file *xyz\_data2.dat*, and apply your linear model to predict the value of $z$ from knowledge of $x$ and $y$.
* Compare your prediction with the known true result. Again, compute $R^2$.

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION_
Again, we have a value of $R^2$ close to unity, and we see that the model
predicts the data well:

@@@CODE-4 src-intro_project/pINTRO_linear_regression.py fromto: Use fitted linear model to predict@-----

# #endif

<% counter += 1 %>
__Part ${counter}.__
<% ex_pythagoras = counter %>
In reality, $z$ was computed as $z=\sqrt{x^2+y^2}$, and the input $x$- and
$y$-values were randomly drawn from the interval $[0,20]$.

* Test your linear model again, but this time draw 50 random $x$- and $y$-values from the interval $[-20,0]$, and compute the true value for $z$ based on that.
* How does your prediction match the real model now, and what does it tell you about the ability of $R^2$ to judge the appropriateness of model selection; in this case, an assumed linear dependency of $z$ on $x$ and $y$?

<% counter += 1 %>
__Part ${counter}.__
* What happens if you redo Part ${ex_pythagoras}, but switch the signs of the obtained regression coefficients?
* Why?

Hint: Taylor expand the multivariable function $g(x,y)=\sqrt{x^2+y^2}$ around
a point $(x_0,y_0)$, and consider, e.g., what happens when $x_0=y_0$
## Hint 1: Compare the curve $z=\sqrt{x^2+y^2}$ with the plane $z=(x+y)/\sqrt{2}$. Where do they seem to agree the most?

# #ifdef SOLUTIONS

_POSSIBLE SOLUTION TO LAST TWO EXERCISES_

Since we use negative values, $z$ will now *decrease* as either $x$ or $y$
is increased towards zero, hence the slope of the 'best fit line' would
have to change sign... We clearly see the mismatch in the following plot:

@@@CODE-4 src-intro_project/pINTRO_linear_regression.py fromto: Draw random x and y values that are negative@-----

The results can be understood better by doing a Taylor expansion of the
function $g(x,y)$ defined in Exercise ${exercise_plot_func}:
!bt
\begin{align*}
g(x,y) &\approx g(x_0,y_0)+\frac{\partial g}{\partial x}(x_0,y_0)(x-x_0)+
\frac{\partial g}{\partial y}(x_0,y_0)(y-y_0) \\
&= \sqrt{x_{0}^2+y_{0}^2}+\frac{x_0}{\sqrt{x_{0}^2+y_{0}^2}}(x-x_0)+
\frac{y_0}{\sqrt{x_{0}^2+y_{0}^2}}(y-y_0) \\
&= \frac{x_0}{\sqrt{x_{0}^2+y_{0}^2}}\cdot{x}+
\frac{y_0}{\sqrt{x_{0}^2+y_{0}^2}}\cdot{y}\,.
\end{align*}
!et

If $x_0=y_0$ *and* both numbers are positive, this reduces to:
!bt
\begin{align*}
g(x,y) \approx \frac{x+y}{\sqrt{2}} \approx 0.7x+0.7y\,,
\end{align*}
!et
which is fairly close to the linear model we found using regression analysis.

However, when we approximate the true function by a plane like this, we can
only expect to get accurate results in the vicinity of the point we started
out with. Thus, if either $x_0$ and $y_0$ are very different from each other,
or one/both of them are negative, the fit we obtained will not be expected to
hold. Indeed, for the case in which $x_0=y_0$, but both are negative, the
Taylor expansion yields:
!bt
\begin{align*}
g(x,y) \approx -\frac{x+y}{\sqrt{2}} \approx -0.7x-0.7y\,.
\end{align*}
!et

# #endif

======= Guidelines for project submission =======
The assignment is provided both as a PDF, and as a Jupyter notebook.
However, the work done to answer the exercises only has to be handed in as a
notebook, though you can submit an additional PDF if you want.
You should bear the following points in mind when working on the project:
* Start your notebook by providing a short introduction in which you outline the nature of the problem(s) to be investigated.
* End your notebook with a brief summary of what you feel you learnt from the project (if anything). Also, if you have any general comments or suggestions for what could be improved in future assignments, this is the place to do it.
* All code that you make use of should be present in the notebook, and it should ideally execute without any errors (especially run-time errors). If you are not able to fix everything before the deadline, you should give your best understanding of what is not working, and how you might go about fixing it.
* If you use an algorithm that is not fully described in the assignment text, you should try to explain it in your own words. This also applies if the method is described elsewhere in the course material.
* In some cases it may suffice to explain your work via comments in the code itself, but other times you might want to include a more elaborate explanation in terms of, e.g., mathematics and/or pseudocode.
* In general, it is a good habit to comment your code (though it can be overdone).
* When working with approximate solutions to equations, it is always useful to check your results against known exact (analytical) solutions, should they be available.
* It is also a good test of a model implementation to study what happens at known 'edge cases'.
* Any figures you include should be easily understandable. You should label axes appropriately, and depending on the problem, include other legends etc. Also, you should discuss your figures in the main text.
* It is always good if you can reflect a little bit around *why* you see what you see.

##======= Bibliography =======
##BIBFILE: papers.pub

